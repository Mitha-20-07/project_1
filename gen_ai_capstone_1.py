# -*- coding: utf-8 -*-
"""Gen_AI_Capstone_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V7z7AkmiZ6PM0qLio5GLGSIO3H67EsQ9
"""

!pip install pymupdf faiss-cpu sentence-transformers groq

from google.colab import files
import fitz  # PyMuPDF

uploaded = files.upload()
pdf_path = next(iter(uploaded))



# Extract text
doc = fitz.open("Abstract ID FIBT306.pdf")
text = ""
for page in doc:
    text += page.get_text()
doc.close()

import re

def chunk_text(text, words_per_chunk=100):
    words = text.split()
    return [' '.join(words[i:i+words_per_chunk]) for i in range(0, len(words), words_per_chunk)]

chunks = chunk_text(text)

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Load embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings
embeddings = model.encode(chunks)

# Create FAISS index
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

def search(query, top_k=5):
    query_vec = model.encode([query])
    distances, indices = index.search(np.array(query_vec), top_k)
    results = [chunks[i] for i in indices[0]]
    return results

import os
import requests

GROQ_API_KEY = " "  # <-- Replace with your real key

def generate_answer(prompt, model="llama3-8b-8192"):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7
    }
    response = requests.post(url, headers=headers, json=data)
    return response.json()['choices'][0]['message']['content']

# Ask a question
query = input("Enter your question: ")
relevant_chunks = search(query)

# Combine context and query
context = "\n".join(relevant_chunks)
prompt = f"Context:\n{context}\n\nQuestion: {query}"

# Get answer from LLM
answer = generate_answer(prompt)
print("Answer:", answer)